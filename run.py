import argparse
from typing import Generator
import numpy as np
import os
import random
import sys
import torch
import torch.nn as nn

from datasets.dataset import TurbDataset, ValidDataset
from models import Generator, weights_init
from utils import computeLR, makeDirs, imageOut

def main(args=None):

    seed = args.manual_seed
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == "cuda":
        print(torch.cuda.get_device_name(0))
        print('Memory Usage:')
        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')
        torch.cuda.manual_seed_all(seed)

    batch_size = args.batch_size
    lrG = args.lr
    channelExponent = args.channel_exponenet
    epochs = args.num_epochs
    dataDir = args.data_dir + "train/"
    dataDirTest = args.data_dir + "test/"

    image_height = 128
    image_width = 128
    image_channels = 3
    beta1 = 0.5
    beta2 = 0.999
    weight_decay = 0.0
    decayLR = True
    prop = None
    saveL1 = False
    dropout = 0.0

    # Data setup

    # print(len(os.listdir(dataDir)))
    # print(len(os.listdir(dataDirTest)))
    training_data = TurbDataset(prop, shuffle=1, mode=0, dataDir=dataDir, dataDirTest=dataDirTest, normMode=1)
    test_data = TurbDataset(prop, shuffle=1, mode=2, dataDir=dataDir, dataDirTest=dataDirTest, normMode=1)

    trainLoader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)
    print(f"Training batches: {len(trainLoader)}")

    dataValidation = ValidDataset(training_data)
    validLoader = torch.utils.data.DataLoader(dataValidation, batch_size=batch_size, shuffle=False, drop_last=True)
    print(f"Validation batches: {len(validLoader)}")

    # Training Setup

    netG = Generator(channelExponent=channelExponent, dropout=dropout)
    nn_parameters = filter(lambda p: p.requires_grad, netG.parameters())
    params = sum([np.prod(p.size()) for p in nn_parameters])

    print(f"Trainable params: {params}.")

    netG.apply(weights_init)
    netG.to(device)

    criterionL1 = nn.L1Loss()
    criterionL1.to(device)

    optimizerG = torch.optim.Adam(netG.parameters(), lr=lrG, betas=(beta1, beta2), weight_decay=weight_decay)

    inputs = torch.FloatTensor(batch_size, image_channels, image_height, image_width)
    inputs = inputs.to(device)

    targets = torch.FloatTensor(batch_size, image_channels, image_height, image_width)
    targets = targets.to(device)

    print(device)
    print(inputs.device)
    print(targets.device)

    history_L1 = []
    history_L1val = []

    print("Training the network")

    for epoch in range(epochs):
        netG.train()
        L1_accum = 0.0
        for i, traindata in enumerate(trainLoader, 0):
            inputs_cpu, targets_cpu = traindata
            inputs_cpu, targets_cpu = inputs_cpu.float().to(device), targets_cpu.float().to(device)
            inputs.data.resize_as_(inputs_cpu).copy_(inputs_cpu)
            targets.data.resize_as_(targets_cpu).copy_(targets_cpu)

            # compute LR decay
            if decayLR:
                currLR = computeLR(epoch, epochs, lrG * 0.1, lrG)
                if currLR < lrG:
                    for g in optimizerG.param_groups:
                        g["lr"] = currLR

            netG.zero_grad()
            gen_out = netG(inputs)

            lossL1 = criterionL1(gen_out, targets)
            lossL1.backward()
            optimizerG.step()
            L1_accum += lossL1.item()

        # validation
        netG.eval()
        with torch.no_grad():
            L1val_accum = 0.0
            for i, validdata in enumerate(validLoader, 0):
                inputs_cpu, targets_cpu = validdata
                inputs_cpu, targets_cpu = inputs_cpu.float().to(device), targets_cpu.float().to(device)
                inputs.resize_as_(inputs_cpu).copy_(inputs_cpu)
                targets.resize_as_(targets_cpu).copy_(targets_cpu)

                outputs = netG(inputs)
                outputs_cpu = outputs.data.cpu().numpy()

                lossL1val = criterionL1(outputs, targets)
                L1val_accum += lossL1val.item()

                if i == 0:
                    input_ndarray = inputs_cpu.cpu().numpy()[0]
                    v_norm = (
                        np.max(np.abs(input_ndarray[0, :, :])) ** 2 + np.max(np.abs(input_ndarray[1, :, :])) ** 2
                    ) ** 0.5
                    outputs_denormalized = training_data.denormalize(outputs_cpu[0], v_norm)
                    targets_denormalized = training_data.denormalize(targets_cpu.cpu().numpy()[0], v_norm)
                    makeDirs(["results_train"])
                    imageOut(
                        f"results_train/epoch{epoch}_{i}", outputs_denormalized, targets_denormalized, saveTargets=True
                    )

        history_L1.append(L1_accum / len(trainLoader))
        history_L1val.append(L1val_accum / len(validLoader))

        print(f"Epoch: {epoch}, L1 train: {history_L1[-1]}, L1 validation: {history_L1val[-1]}")

        if (epoch + 1) % 20 == 0:
            torch.save(
                {
                    "epoch": epoch,
                    ",odel_state_dict": netG.state_dict(),
                    "optimizer_state_dict": optimizerG.state_dict(),
                    "loss": lossL1,
                },
                f"newtork_expo_{channelExponent}_{epoch}.pth",
            )

    torch.save(netG.state_dict(), "modelG.pth")
    print("Completed Training and saved trained model.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size", type=int, default=10, help="Batch Size")
    parser.add_argument("--channel_exponenet", type=int, default=5, help="Channel Exponent")
    parser.add_argument("--data_dir", type=str, default="data/", help="Path to data directory")
    parser.add_argument("--num_epochs", type=int, default=10, help="Number of Epochs")
    parser.add_argument("--lr", type=float, default=0.0006, help="Learning Rate")
    parser.add_argument("--manual_seed", type=int, default=42, help="Initial Seed")
    args = parser.parse_args()
    main(args)
